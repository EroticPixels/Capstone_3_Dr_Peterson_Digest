{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/deanchen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Load txt files\n",
    "#Seperate the text into documents, 25 sentences as a document\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def divide_into_smaller_documents(text, max_sentences):\n",
    "    sentences = sent_tokenize(text)\n",
    "    num_sentences = len(sentences)\n",
    "    num_documents = num_sentences // max_sentences + 1\n",
    "\n",
    "    smaller_documents = []\n",
    "    start_idx = 0\n",
    "    for i in range(num_documents):\n",
    "        end_idx = min(start_idx + max_sentences, num_sentences)\n",
    "        smaller_doc = sentences[start_idx:end_idx]\n",
    "        smaller_documents.append(smaller_doc)\n",
    "        start_idx = end_idx\n",
    "\n",
    "    return smaller_documents\n",
    "\n",
    "# Load text files\n",
    "file_paths = [\"Aspen_Barricades_of_Culture_Wars.txt\", \"Douglas_Murray_IQ_Politics_and_the_Left.txt\", \"Nina_Paley_Animator_Extraordinaire.txt\", \"DR_Iain_McGilchrist.txt\"]\n",
    "texts = []\n",
    "for file_path in file_paths:\n",
    "    with open(file_path, \"r\") as file:\n",
    "        texts.append(file.read())\n",
    "\n",
    "# Divide into smaller documents with 25 sentences each\n",
    "max_sentences_per_document = 25\n",
    "all_smaller_documents = []\n",
    "for text in texts:\n",
    "    smaller_documents = divide_into_smaller_documents(text, max_sentences_per_document)\n",
    "    all_smaller_documents.extend(smaller_documents)\n",
    "\n",
    "# Load every document into a NumPy array\n",
    "num_documents = len(all_smaller_documents)\n",
    "array_documents = np.empty(num_documents, dtype=object)\n",
    "for i, doc in enumerate(all_smaller_documents):\n",
    "    array_documents[i] = '\\n'.join(doc)\n",
    "\n",
    "# Save the array as a .npy file\n",
    "np.save(\"array_documents.npy\", array_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr. Jordan Peterson: Well, I think you don’t want to underestimate the role that technological transformation has played in this.\n",
      "I’ve been thinking about YouTube and podcasts quite intensely for about two years.\n",
      "I started putting my university lectures on YouTube in 2013.\n",
      "I did that for a variety of reasons—mostly curiosity and the drive to learn.\n",
      "I’ve found that, if I want to learn a technology, the best way to do it is to use it.\n",
      "I’m always learning new technologies—not that that makes me particularly unique—and I had some success with my lectures on public television in Canada.\n",
      "I did some lectures with this series called \"Big Ideas\" on Canadian public television.\n",
      "There’s about 200 of those lectures, and I did five of them.\n",
      "Two hundred done by 200 different people, but I did five of them, and they were regularly in the top 10 of the most viewed lectures.\n",
      "So I knew there was some broader market for, let’s say, ideas.\n",
      "I thought, \"well, I might as well put my lectures up on YouTube, and see what happens.\"\n",
      "And then, by April of 2016, I had a million views.\n",
      "I thought, \"huh.\n",
      "The only reason people are watching these is because they want to watch them, because they’re actually really hard.\"\n",
      "A million of something is a lot.\n",
      "If you sell a million copies of your book—well, first of all, that never happens, right?\n",
      "It’s very, very rare.\n",
      "You never have your scientific papers cited a million times.\n",
      "You rarely have a million dollars.\n",
      "It’s a very large number.\n",
      "Well, fair enough.\n",
      "It’s, of course, not as uncommon as it once was, but it’s still a significant number, and I didn’t really have any way of calibrating that.\n",
      "I thought, \"well, what am I supposed to do, now that I hit a million views?\n",
      "How am I supposed to conceptualize that?\n",
      "What is this YouTube thing anyways, that was once a repository for cute cat videos?\n"
     ]
    }
   ],
   "source": [
    "print(array_documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(array_documents)):\n",
    "    document = array_documents[i]\n",
    "    document = document.replace(\"Dr. Jordan Peterson: \", \"\")\n",
    "    document = document.replace(\"JP: \", \"\")\n",
    "    array_documents[i] = document\n",
    "\n",
    "# Save the modified array back to the original file\n",
    "np.save(\"array_documents.npy\", array_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well, I think you don’t want to underestimate the role that technological transformation has played in this.\n",
      "I’ve been thinking about YouTube and podcasts quite intensely for about two years.\n",
      "I started putting my university lectures on YouTube in 2013.\n",
      "I did that for a variety of reasons—mostly curiosity and the drive to learn.\n",
      "I’ve found that, if I want to learn a technology, the best way to do it is to use it.\n",
      "I’m always learning new technologies—not that that makes me particularly unique—and I had some success with my lectures on public television in Canada.\n",
      "I did some lectures with this series called \"Big Ideas\" on Canadian public television.\n",
      "There’s about 200 of those lectures, and I did five of them.\n",
      "Two hundred done by 200 different people, but I did five of them, and they were regularly in the top 10 of the most viewed lectures.\n",
      "So I knew there was some broader market for, let’s say, ideas.\n",
      "I thought, \"well, I might as well put my lectures up on YouTube, and see what happens.\"\n",
      "And then, by April of 2016, I had a million views.\n",
      "I thought, \"huh.\n",
      "The only reason people are watching these is because they want to watch them, because they’re actually really hard.\"\n",
      "A million of something is a lot.\n",
      "If you sell a million copies of your book—well, first of all, that never happens, right?\n",
      "It’s very, very rare.\n",
      "You never have your scientific papers cited a million times.\n",
      "You rarely have a million dollars.\n",
      "It’s a very large number.\n",
      "Well, fair enough.\n",
      "It’s, of course, not as uncommon as it once was, but it’s still a significant number, and I didn’t really have any way of calibrating that.\n",
      "I thought, \"well, what am I supposed to do, now that I hit a million views?\n",
      "How am I supposed to conceptualize that?\n",
      "What is this YouTube thing anyways, that was once a repository for cute cat videos?\n"
     ]
    }
   ],
   "source": [
    "print(array_documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "import glob\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/deanchen/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/deanchen/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/deanchen/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/deanchen/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/deanchen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove punctuation\n",
    "    text = \"\".join(char for char in text if char not in string.punctuation)\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Tokenize the text into words\n",
    "    words = text.split()\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Perform POS tagging\n",
    "    tagged_words = nltk.pos_tag(words)\n",
    "\n",
    "    # Lemmatize the words based on POS tags\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word, pos=get_wordnet_pos(tag)) for word, tag in tagged_words]\n",
    "\n",
    "    # Join the processed words back into a single string\n",
    "    processed_text = \" \".join(words)\n",
    "\n",
    "    return processed_text\n",
    "\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    \"\"\"Map POS tag to WordNet POS tag\"\"\"\n",
    "    if tag.startswith('J'):\n",
    "        return 'a'  # Adjective\n",
    "    elif tag.startswith('V'):\n",
    "        return 'v'  # Verb\n",
    "    elif tag.startswith('N'):\n",
    "        return 'n'  # Noun\n",
    "    elif tag.startswith('R'):\n",
    "        return 'r'  # Adverb\n",
    "    else:\n",
    "        return 'n'  # Default to noun\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/deanchen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/deanchen/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "well think don’t want underestimate role technological transformation play i’ve think youtube podcasts quite intensely two year start put university lecture youtube 2013 variety reasons—mostly curiosity drive learn i’ve find want learn technology best way use i’m always learn new technologies—not make particularly unique—and success lecture public television canada lecture series call big idea canadian public television there’s 200 lecture five two hundred do 200 different people five regularly top 10 viewed lecture know broad market let’s say idea think well might well put lecture youtube see happen april 2016 million view think huh reason people watch want watch they’re actually really hard million something lot sell million copy book—well first never happen right it’s rare never scientific paper cite million time rarely million dollar it’s large number well fair enough it’s course uncommon it’s still significant number didn’t really way calibrate think well suppose hit million view suppose conceptualize youtube thing anyways repository cute cat video\n"
     ]
    }
   ],
   "source": [
    "doc_list = array_documents.tolist()\n",
    "doc_list = [preprocess_text(doc) for doc in doc_list]\n",
    "print(doc_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "play ate well\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(preprocess_text('played, ate, better'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(   \n",
    "    lowercase = True,\n",
    "    max_df= .8,\n",
    "    min_df= 5,\n",
    "    ngram_range= (1,3),\n",
    "    max_features= 100\n",
    ")\n",
    "vectors = vectorizer.fit_transform(doc_list)\n",
    "feature_names = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense = vectors.todense()\n",
    "denselist = dense.tolist()\n",
    "\n",
    "all_keywords = []\n",
    "\n",
    "for doc in denselist:\n",
    "    x = 0\n",
    "    keywords = []\n",
    "    for score in doc:\n",
    "        if score > 0:\n",
    "            keywords.append(feature_names[x])\n",
    "        x  = x + 1\n",
    "    all_keywords.append(keywords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well, I think you don’t want to underestimate the role that technological transformation has played in this.\n",
      "I’ve been thinking about YouTube and podcasts quite intensely for about two years.\n",
      "I started putting my university lectures on YouTube in 2013.\n",
      "I did that for a variety of reasons—mostly curiosity and the drive to learn.\n",
      "I’ve found that, if I want to learn a technology, the best way to do it is to use it.\n",
      "I’m always learning new technologies—not that that makes me particularly unique—and I had some success with my lectures on public television in Canada.\n",
      "I did some lectures with this series called \"Big Ideas\" on Canadian public television.\n",
      "There’s about 200 of those lectures, and I did five of them.\n",
      "Two hundred done by 200 different people, but I did five of them, and they were regularly in the top 10 of the most viewed lectures.\n",
      "So I knew there was some broader market for, let’s say, ideas.\n",
      "I thought, \"well, I might as well put my lectures up on YouTube, and see what happens.\"\n",
      "And then, by April of 2016, I had a million views.\n",
      "I thought, \"huh.\n",
      "The only reason people are watching these is because they want to watch them, because they’re actually really hard.\"\n",
      "A million of something is a lot.\n",
      "If you sell a million copies of your book—well, first of all, that never happens, right?\n",
      "It’s very, very rare.\n",
      "You never have your scientific papers cited a million times.\n",
      "You rarely have a million dollars.\n",
      "It’s a very large number.\n",
      "Well, fair enough.\n",
      "It’s, of course, not as uncommon as it once was, but it’s still a significant number, and I didn’t really have any way of calibrating that.\n",
      "I thought, \"well, what am I supposed to do, now that I hit a million views?\n",
      "How am I supposed to conceptualize that?\n",
      "What is this YouTube thing anyways, that was once a repository for cute cat videos?\n",
      "well think don’t want underestimate role technological transformation play i’ve think youtube podcasts quite intensely two year start put university lecture youtube 2013 variety reasons—mostly curiosity drive learn i’ve find want learn technology best way use i’m always learn new technologies—not make particularly unique—and success lecture public television canada lecture series call big idea canadian public television there’s 200 lecture five two hundred do 200 different people five regularly top 10 viewed lecture know broad market let’s say idea think well might well put lecture youtube see happen april 2016 million view think huh reason people watch want watch they’re actually really hard million something lot sell million copy book—well first never happen right it’s rare never scientific paper cite million time rarely million dollar it’s large number well fair enough it’s course uncommon it’s still significant number didn’t really way calibrate think well suppose hit million view suppose conceptualize youtube thing anyways repository cute cat video\n",
      "['actually', 'and', 'book', 'call', 'course', 'don', 'first', 'happen', 'idea', 'know', 'learn', 'let', 'let say', 'lot', 'make', 'might', 'people', 'put', 'quite', 'really', 'reason', 'right', 'see', 'something', 'start', 'there', 'they', 'they re', 'time', 'two', 'university', 'use', 've', 'want', 'watch', 'way', 'year']\n"
     ]
    }
   ],
   "source": [
    "print(array_documents[0])\n",
    "print(doc_list[0])\n",
    "print(all_keywords[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_k = 7\n",
    "model = KMeans(n_clusters = true_k, init = 'k-means++', max_iter = 100, n_init=1)\n",
    "model.fit(vectors)\n",
    "\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1] \n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "with open ('sample_topic_result_7.txt', 'w', encoding = 'utf-8') as f:\n",
    "    for i in range(true_k):\n",
    "        f.write(f\"\\nCluster {i}: \")\n",
    "        for ind in order_centroids[i, :10]:\n",
    "            f.write(f'{terms[ind]} ')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69\n"
     ]
    }
   ],
   "source": [
    "'''Choosing 20 clusters did not yield an intelligible result. Here, as a reminder,\n",
    "we have only 69 documents at hand, so 20 clusters might be too many. Let's experiment with\n",
    "10 and 7 clusters'''\n",
    "\n",
    "\n",
    "print(array_documents.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "kmean_indices = model.fit_predict(vectors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
